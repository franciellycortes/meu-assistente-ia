import streamlit as st
import google.generativeai as genai
from PIL import Image

# 1. CONFIGURA√á√ÉO DA P√ÅGINA
st.set_page_config(page_title="Mentor Neuropsicopedag√≥gico", page_icon="üß†", layout="wide")

# 2. PERSONALIDADE T√âCNICA (INSTRU√á√ÉO DE SISTEMA)
instrucao_sistema = """
Voc√™ √© um Mentor de Alto N√≠vel em Psicopedagogia Cl√≠nica, fundamentado na Epistemologia Convergente (Jorge Visca).
Integre: Piaget (Cogni√ß√£o), Vygotsky (ZDP), Wallon (Afetividade), Alicia Fern√°ndez (Desejo de Aprender) e Neuroci√™ncias (DSM-5-TR).

ESTRUTURA DE RESPOSTA:
1. Eixo Cognitivo: Est√°gio e fun√ß√µes executivas.
2. Eixo Socioafetivo: Media√ß√£o e v√≠nculo com o saber.
3. Eixo Instrumental: Sugest√£o de testes (EOCA, Provas Operat√≥rias).
4. Eixo Terap√™utico: Hip√≥teses e estrat√©gias pr√°ticas.
"""

# 3. CONFIGURA√á√ÉO DO MODELO COM GOOGLE SEARCH
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    
    # Criamos o modelo chamando o nome correto para evitar o erro 404
    # Adicionamos a ferramenta de pesquisa (Google Search) aqui
    model = genai.GenerativeModel(
        model_name='gemini-2.0-flash',
        system_instruction=instrucao_sistema,
        tools=[{"google_search_retrieval": {}}] 
    )
except Exception as e:
    st.error(f"Erro na configura√ß√£o: {e}")

# 4. GEST√ÉO DE MEM√ìRIA (CONTEXTO)
if "chat_session" not in st.session_state:
    st.session_state.chat_session = model.start_chat(history=[])

# 5. BARRA LATERAL COMPLETA
with st.sidebar:
    st.title("üìÇ Central de Intelig√™ncia")
    st.info("Modelo: Gemini 2.0 Flash (Gera√ß√£o 3)")
    
    arquivo_upload = st.file_uploader("Subir Relat√≥rio ou Imagem", type=["png", "jpg", "jpeg", "pdf"])
    
    st.divider()
    if st.button("üóëÔ∏è Limpar Contexto (Nova Supervis√£o)"):
        st.session_state.chat_session = model.start_chat(history=[])
        st.rerun()

st.title("üß† Mentor Neuropsicopedag√≥gico")

# 6. EXIBI√á√ÉO DO HIST√ìRICO
for mensagem in st.session_state.chat_session.history:
    role = "user" if mensagem.role == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(mensagem.parts[0].text)

# 7. INTERA√á√ÉO E PROCESSAMENTO
if prompt := st.chat_input("Descreva o caso cl√≠nico..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    
    try:
        conteudo = [prompt]
        if arquivo_upload:
            if arquivo_upload.type == "application/pdf":
                conteudo.append({"mime_type": "application/pdf", "data": arquivo_upload.read()})
            else:
                conteudo.append(Image.open(arquivo_upload))

        # O modelo processar√° o prompt usando a mem√≥ria e a pesquisa em tempo real
        response = st.session_state.chat_session.send_message(conteudo)
        
        with st.chat_message("assistant"):
            st.markdown(response.text)
            
    except Exception as e:
        if "429" in str(e):
            st.warning("Limite de cota do Gemini 2.0 atingido. Aguarde 60 segundos.")
        elif "404" in str(e):
            st.error("Erro 404: O modelo n√£o suportou esta combina√ß√£o de ferramentas no momento.")
        else:
            st.error(f"Ocorreu um erro: {e}")
